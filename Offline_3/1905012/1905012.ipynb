{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def forward(self, z):\n",
    "        self.output = np.maximum(0, z)\n",
    "        return self.output\n",
    "    def backward(self, grad):\n",
    "        return grad * (self.output > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def forward(self, x):\n",
    "        exps = np.exp(x - np.max(x, axis=1, keepdims=True)) # naile overflow hoi\n",
    "        self.output = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "        return self.output\n",
    "    def backward(self, grad):\t\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters for Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_params:\n",
    "    def __init__(self, momentum=0.9, epsilon=1e-5):\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = None\n",
    "        self.beta = None\n",
    "\n",
    "    def build(self, units):\n",
    "        self.gamma = np.ones((1, units))\n",
    "        self.beta = np.zeros((1, units))\n",
    "\n",
    "    def update_params(self, learning_rate, grad_gamma, grad_beta):\n",
    "        \n",
    "        self.gamma -= learning_rate * grad_gamma\n",
    "        self.beta -= learning_rate * grad_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Normalization Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_Stats:\n",
    "    def __init__(self):\n",
    "        self.batch_mean = 0\n",
    "        self.batch_var =0\n",
    "    def update_training(self, x):\n",
    "        self.batch_mean = np.mean(x, axis=0, keepdims=True)\n",
    "        self.batch_var = np.var(x, axis=0, keepdims=True)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch_running:\n",
    "    def __init__(self):\n",
    "        self.running_mean = None\n",
    "        self.running_var = None\n",
    "\n",
    "    def build(self, units):\n",
    "        # running mean and var\n",
    "        self.running_mean = np.zeros((1, units))\n",
    "        self.running_var = np.ones((1, units))\n",
    "\n",
    "    def update_training(self, params, batch_stats):\n",
    "        # Updating the running mean and var here \n",
    "        self.running_mean = params.momentum * self.running_mean + (1 - params.momentum) * batch_stats.batch_mean\n",
    "        self.running_var = params.momentum * self.running_var + (1 - params.momentum) * batch_stats.batch_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalization:\n",
    "    def __init__(self, momentum=0.9, epsilon=1e-5):\n",
    "        self.params = Batch_params(momentum, epsilon)\n",
    "        self.running_stats = Batch_running()\n",
    "        self.batch_stats = Batch_Stats()\n",
    "\n",
    "    def build(self, units):\n",
    "        self.params.build(units)\n",
    "        self.running_stats.build(units)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if training:\n",
    "            self.batch_stats.update_training(x)\n",
    "            self.running_stats.update_training(self.params, self.batch_stats)\n",
    "            # Normalizing \n",
    "            self.x_centered = x - self.batch_stats.batch_mean\n",
    "            self.std_inv = 1.0 / np.sqrt(self.batch_stats.batch_var + self.params.epsilon)\n",
    "            self.x_normalized = self.x_centered * self.std_inv\n",
    "        else:\n",
    "            # for inference\n",
    "            self.x_centered = x - self.running_stats.running_mean\n",
    "            self.std_inv = 1.0 / np.sqrt(self.running_stats.running_var + self.params.epsilon)\n",
    "            self.x_normalized = self.x_centered * self.std_inv\n",
    "\n",
    "        # Scaling , shifting here\n",
    "        self.out = self.params.gamma * self.x_normalized + self.params.beta\n",
    "        return self.out\n",
    "\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        batch_size = grad_output.shape[0]\n",
    "        self.update_gradients(grad_output)\n",
    "\n",
    "        # Gradient w.r.t. normalized input\n",
    "        grad_x_normalized = grad_output * self.params.gamma\n",
    "        grad_var = np.sum(grad_x_normalized * self.x_centered * -0.5 * self.std_inv**3, axis=0, keepdims=True)\n",
    "\n",
    "        # Gradient w.r.t. mean\n",
    "        grad_mean = np.sum(grad_x_normalized * -self.std_inv, axis=0, keepdims=True) + grad_var * np.sum(-2 * self.x_centered, axis=0, keepdims=True) / batch_size\n",
    "        grad_input = grad_x_normalized * self.std_inv + grad_var * 2 * self.x_centered / batch_size + grad_mean / batch_size\n",
    "\n",
    "        return grad_input\n",
    "    \n",
    "    def update_gradients(self, grad_output):\n",
    "        self.grad_gamma = np.sum(grad_output * self.x_normalized, axis=0, keepdims=True)\n",
    "        self.grad_beta = np.sum(grad_output, axis=0, keepdims=True)\n",
    "    \n",
    "\n",
    "    def update_weights(self, learning_rate):\n",
    "        self.params.update_params(learning_rate, self.grad_gamma, self.grad_beta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, input_dim, node_num, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8):\n",
    "        # individual node weights and bias\n",
    "        self.weights = np.random.randn(input_dim) * 0.01\n",
    "        self.bias = 0\n",
    "        self.node_num = node_num  # reference\n",
    "\n",
    "        self.m_w = np.zeros_like(self.weights)  # First moment for weights\n",
    "        self.v_w = np.zeros_like(self.weights)  # Second moment for weights\n",
    "        self.m_b = 0                            # First moment for bias\n",
    "        self.v_b = 0                            # Second moment for bias\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  \n",
    "        \n",
    "\n",
    "    def update(self, all_weights, all_biases):\n",
    "        self.weights = all_weights[:, self.node_num]\n",
    "        self.bias = all_biases[0, self.node_num]\n",
    "\n",
    "    \n",
    "    def update_adam(self, grad_weights, grad_bias, t):\n",
    "        self.t = t\n",
    "        \n",
    "        # Updating first and second moment estimates for weights\n",
    "        self.m_w = self.beta_1 * self.m_w + (1 - self.beta_1) * grad_weights\n",
    "        self.v_w = self.beta_2 * self.v_w + (1 - self.beta_2) * np.square(grad_weights)\n",
    "        \n",
    "        # Updating first and second moment estimates for bias\n",
    "        self.m_b = self.beta_1 * self.m_b + (1 - self.beta_1) * grad_bias\n",
    "        self.v_b = self.beta_2 * self.v_b + (1 - self.beta_2) * np.square(grad_bias)\n",
    "\n",
    "        # Bias correction step -----------------------------------------------\n",
    "        m_hat_w = self.m_w / (1 - self.beta_1 ** self.t)\n",
    "        v_hat_w = self.v_w / (1 - self.beta_2 ** self.t)\n",
    "        m_hat_b = self.m_b / (1 - self.beta_1 ** self.t)\n",
    "        v_hat_b = self.v_b / (1 - self.beta_2 ** self.t)\n",
    "\n",
    "        # using Adam Optimizer\n",
    "        self.weights -= self.learning_rate * m_hat_w / (np.sqrt(v_hat_w) + self.epsilon)\n",
    "        self.bias -= self.learning_rate * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Dense:\n",
    "    def __init__(self, units, activation=None, batch_normalization=True, dropout_rate=0.0, learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8):\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.input_dim = None  # the first forward pass e dekhboni\n",
    "        if batch_normalization:\n",
    "            self.batch_norm = BatchNormalization()\n",
    "        else:\n",
    "            self.batch_norm = None\n",
    "\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0 \n",
    "\n",
    "\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout_mask = None\n",
    "        self.nodes=[]\n",
    "\n",
    "    def build(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        np.random.seed(296)\n",
    "        self.nodes = [Node(input_dim, i, self.learning_rate, self.beta_1, self.beta_2, self.epsilon) for i in range(self.units)]\n",
    "\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            self.activation_fn = ReLU()\n",
    "        elif self.activation == 'softmax':\n",
    "            self.activation_fn = Softmax()\n",
    "        else:\n",
    "            self.activation_fn = None\n",
    "\n",
    "        if self.batch_norm is not None:\n",
    "            self.batch_norm.build(self.units) # BATCH NORMALIZATION HERE ----------\n",
    "\n",
    "        # print(self.weights.shape, self.bias.shape)\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        if self.input_dim is None:\n",
    "            self.build(x.shape[1])  # first pass e dekhboni\n",
    "        self.input = x\n",
    "\n",
    "        assembled_weights = np.array([node.weights for node in self.nodes]).T  # Shape (input_dim, units)\n",
    "        assembled_bias = np.array([node.bias for node in self.nodes]).reshape(1, -1)\n",
    "        self.output = np.dot(x, assembled_weights) + assembled_bias\n",
    "\n",
    "        if self.batch_norm is not None:\n",
    "            self.output = self.batch_norm.forward(self.output, training=training)\n",
    "\n",
    "        if training and self.dropout_rate > 0:\n",
    "            np.random.seed(246)\n",
    "            self.dropout_mask = (np.random.rand(*self.output.shape) > self.dropout_rate).astype(np.float32)\n",
    "            self.output = self.output * self.dropout_mask / (1 - self.dropout_rate)\n",
    "        \n",
    "        if self.activation_fn:\n",
    "            self.output = self.activation_fn.forward(self.output)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, grad):\n",
    "        if self.activation_fn:\n",
    "            grad = self.activation_fn.backward(grad)\n",
    "\n",
    "        # forward pass a dropout thakle oita handle korbo\n",
    "        if self.dropout_rate > 0:\n",
    "            grad = grad * self.dropout_mask / (1 - self.dropout_rate)\n",
    "        \n",
    "        if self.batch_norm is not None:\n",
    "            grad = self.batch_norm.backward(grad)\n",
    "\n",
    "        assembled_weights = np.array([node.weights for node in self.nodes]).T\n",
    "        assembled_bias = np.array([node.bias for node in self.nodes]).reshape(1, -1)\n",
    "\n",
    "\n",
    "        #  gradients of weights and bias\n",
    "        self.grad_weights = np.dot(self.input.T, grad) / self.input.shape[0]\n",
    "        self.grad_bias = np.sum(grad, axis=0, keepdims=True) / self.input.shape[0]\n",
    "        grad_input = np.dot(grad, assembled_weights.T)\n",
    "        \n",
    "        return grad_input\n",
    "\n",
    "    def update_weights(self, learning_rate = 0.005):\n",
    "\n",
    "        self.t += 1\n",
    "        # Updating weights and biases using Adam optimizer in nodes\n",
    "        for i, node in enumerate(self.nodes):\n",
    "            node.update_adam(self.grad_weights[:, i], self.grad_bias[:, i], self.t)\n",
    "\n",
    "\n",
    "        if self.batch_norm:\n",
    "            self.batch_norm.update_weights(self.learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Simple Update (Not ADAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assembled_weights -= self.learning_rate * self.grad_weights\n",
    "# assembled_bias = assembled_bias.astype(np.float64)\n",
    "# assembled_bias -= self.learning_rate * self.grad_bias\n",
    "\n",
    "# After updating weights and biases, update nodes with the new values\n",
    "# for i, node in enumerate(self.nodes):\n",
    "#     node.update(assembled_weights, assembled_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function (Cross-Entropy Loss)\n",
    "class CrossEntropyLoss:\n",
    "    def forward(self, predictions, targets):\n",
    "        predictions = np.clip(predictions, 1e-12, 1 - 1e-12)\n",
    "        return -np.mean(np.sum(targets * np.log(predictions), axis=1))\n",
    "\n",
    "    def backward(self, predictions, targets):\n",
    "        return predictions - targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Sequential Model class\n",
    "class Sequential:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.loss_fn = None\n",
    "        self.learning_rate = None\n",
    "\n",
    "    def compile(self, loss, optimizer, learning_rate):\n",
    "        # Set loss function and optimizer\n",
    "        if loss == 'cross_entropy':\n",
    "            self.loss_fn = CrossEntropyLoss()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def forward(self, x, training=True):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x, training=training)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        for layer in reversed(self.layers):\n",
    "            grad = layer.backward(grad)\n",
    "\n",
    "    def update_weights(self):\n",
    "        for layer in self.layers:\n",
    "            if hasattr(layer, 'update_weights'):\n",
    "                layer.update_weights(self.learning_rate)\n",
    "\n",
    "    def fit(self, X, y, epochs, batch_size):\n",
    "        for epoch in range(epochs):\n",
    "            indices = np.arange(X.shape[0])\n",
    "            np.random.seed(148)\n",
    "            np.random.shuffle(indices)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            epoch_loss = 0\n",
    "            correct_predictions = 0\n",
    "\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i + batch_size]\n",
    "                y_batch = y_shuffled[i:i + batch_size]\n",
    "\n",
    "               \n",
    "                predictions = self.forward(X_batch, training=True)\n",
    "\n",
    "                #  loss\n",
    "                loss = self.loss_fn.forward(predictions, y_batch)\n",
    "                epoch_loss += loss\n",
    "\n",
    "                # Acc\n",
    "                predicted_labels = np.argmax(predictions, axis=1)\n",
    "                true_labels = np.argmax(y_batch, axis=1)\n",
    "                correct_predictions += np.sum(predicted_labels == true_labels)\n",
    "\n",
    "                # Back pass\n",
    "                grad = self.loss_fn.backward(predictions, y_batch)\n",
    "                self.backward(grad)\n",
    "\n",
    "                # Update weights\n",
    "                self.update_weights() # adam now\n",
    "\n",
    "            epoch_loss /= (X.shape[0] // batch_size)\n",
    "            accuracy = correct_predictions / X.shape[0]\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass without training\n",
    "        predictions = self.forward(X, training=False)\n",
    "        # floating probabilities to class predictions\n",
    "        return np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining three model architectures\n",
    "def build_model(config):\n",
    "    if config == 1:\n",
    "        return Sequential([\n",
    "            Dense(units=128, activation='relu', dropout_rate=0.4),\n",
    "            Dense(units=64, activation='relu', dropout_rate=0.3),\n",
    "            Dense(units=32, activation='relu', dropout_rate=0.2),\n",
    "            Dense(units=10, activation='softmax')\n",
    "        ])\n",
    "    elif config == 2:\n",
    "        return Sequential([\n",
    "            Dense(units=256, activation='relu', dropout_rate=0.5),\n",
    "            Dense(units=128, activation='relu', dropout_rate=0.4),\n",
    "            Dense(units=10, activation='softmax')\n",
    "        ])\n",
    "    elif config == 3:\n",
    "        return Sequential([\n",
    "            Dense(units=256, activation='relu', dropout_rate=0.5),\n",
    "            Dense(units=64, activation='relu', dropout_rate=0.2),\n",
    "            Dense(units=32, activation='relu', dropout_rate=0.2),\n",
    "            Dense(units=16, activation='relu', dropout_rate=0.1),\n",
    "            Dense(units=10, activation='softmax')\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV files into DataFrames\n",
    "import pandas as pd\n",
    "x_train = pd.read_csv(\"x_train.csv\", header=None).values\n",
    "y_train = pd.read_csv(\"y_train.csv\", header=None).values\n",
    "x_test = pd.read_csv(\"x_test.csv\", header=None).values\n",
    "y_test = pd.read_csv(\"y_test.csv\", header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(120)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=10000, random_state=12)\n",
    "x_train.shape, x_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.005, 0.01, 0.02]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "best_config = None\n",
    "\n",
    "for config_num in range(1, 4):\n",
    "\tfor lr in learning_rates:\n",
    "\t\tmodel = build_model(config_num)\n",
    "\t\tmodel.compile(loss='cross_entropy', optimizer='Adam', learning_rate=lr)\n",
    "\t\tmodel.fit(x_train, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "\t\tpredictions = model.predict(x_val)\n",
    "\t\ttest_accuracy = np.mean(predictions == np.argmax(y_val, axis=1))\n",
    "\n",
    "\n",
    "\t\tif test_accuracy > best_accuracy:\n",
    "\t\t\tbest_accuracy = test_accuracy\n",
    "\t\t\tbest_model = model\n",
    "\t\t\tbest_config = (config_num, lr)\n",
    "\t\t\n",
    "\t\ttrue_labels = np.argmax(y_val, axis=1)\n",
    "\t\tpredicted_labels = predictions\n",
    "\n",
    "\t\tconf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\t\tconf_matrix_filename = f'confusion_matrix_model_{config_num}_lr_{lr}.csv'\n",
    "\t\tnp.savetxt(conf_matrix_filename, conf_matrix, delimiter=',')\n",
    "\n",
    "\t\tprecision = precision_score(true_labels, predicted_labels, average=None)\n",
    "\t\trecall = recall_score(true_labels, predicted_labels, average=None)\n",
    "\t\tf1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "\n",
    "\t\tprecision_macro = precision_score(true_labels, predicted_labels, average='macro')\n",
    "\t\trecall_macro = recall_score(true_labels, predicted_labels, average='macro')\n",
    "\t\tf1_macro = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "\t\tmetrics_filename = f'metrics_model_{config_num}_lr_{lr}.csv'\n",
    "\t\twith open(metrics_filename, 'w', newline='') as file:\n",
    "\t\t\twriter = csv.writer(file)\n",
    "    \t\t# Write header\n",
    "\t\t\twriter.writerow(['Class', 'Precision', 'Recall', 'F1-score'])\n",
    "    \n",
    "    \t\t# Write class-level metrics\n",
    "\t\t\tfor i, (p, r, f) in enumerate(zip(precision, recall, f1)):\n",
    "\t\t\t\twriter.writerow([f'Class {i}', p, r, f])\n",
    "    \n",
    "    \t\t# Write macro metrics\n",
    "\t\t\twriter.writerow(['Macro Average', precision_macro, recall_macro, f1_macro])\n",
    "\n",
    "# the best model configuration and accuracy\n",
    "print(f\"Best model configuration: Model {best_config[0]} with learning rate {best_config[1]}\")\n",
    "print(f\"Best accuracy: {best_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture using Sequential\n",
    "model = Sequential([\n",
    "    Dense(units=128, activation='relu', dropout_rate=0.4),\n",
    "    Dense(units=64, activation='relu', dropout_rate=0.3),\n",
    "    Dense(units=32, activation='relu', dropout_rate=0.2),\n",
    "    Dense(units=10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='cross_entropy', optimizer='Adam', learning_rate=0.005)\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=64)\n",
    "\n",
    "predictions = model.predict(x_test)\n",
    "test_accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true labels and predictions\n",
    "true_labels = np.argmax(y_test, axis=1)\n",
    "predicted_labels = predictions\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# precision, recall, and F1-score for 10 class\n",
    "precision = precision_score(true_labels, predicted_labels, average=None)\n",
    "recall = recall_score(true_labels, predicted_labels, average=None)\n",
    "f1 = f1_score(true_labels, predicted_labels, average=None)\n",
    "\n",
    "print(\"\\nPrecision per class:\", precision)\n",
    "print(\"Recall per class:\", recall)\n",
    "print(\"F1-score per class:\", f1)\n",
    "\n",
    "# macro avg\n",
    "precision_macro = precision_score(true_labels, predicted_labels, average='macro')\n",
    "recall_macro = recall_score(true_labels, predicted_labels, average='macro')\n",
    "f1_macro = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "print(\"\\nOverall Precision (Macro):\", precision_macro)\n",
    "print(\"Overall Recall (Macro):\", recall_macro)\n",
    "print(\"Overall F1-score (Macro):\", f1_macro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model\n",
    "weights_data = []\n",
    "for layer in model.layers:\n",
    "    layer_weights = [node.weights for node in layer.nodes]  # Collecting weights for nodes in each layer\n",
    "    layer_biases = [node.bias for node in layer.nodes]      # Collecting biases for nodes in each layer\n",
    "    layer_gamma = layer.batch_norm.params.gamma\n",
    "    layer_beta = layer.batch_norm.params.beta\n",
    "    layer_running_mean = layer.batch_norm.running_stats.running_mean\n",
    "    layer_running_var = layer.batch_norm.running_stats.running_var\n",
    "    layer_epsilon = layer.batch_norm.params.epsilon\n",
    "    weights_data.append({'weights': layer_weights, \n",
    "                         'biases': layer_biases,\n",
    "                         'gamma':layer_gamma,\n",
    "                         'beta':layer_beta,\n",
    "                         'r_mean':layer_running_mean,\n",
    "                         'r_var':layer_running_var,\n",
    "                         'epsilon':layer_epsilon\n",
    "\t\t\t\t\t\t })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights_data using pickle\n",
    "\n",
    "model_data = {\n",
    "    'weights_data': weights_data,\n",
    "    'best_config': best_config\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "with open('model_1905012.pkl', 'wb') as file:\n",
    "    pickle.dump(model_data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading weights\n",
    "\n",
    "with open('model_1905012.pkl', 'rb') as file:\n",
    "    model_data = pickle.load(file)\n",
    "\n",
    "\n",
    "weights_data = model_data['weights_data']\n",
    "best_config = model_data['best_config']\n",
    "\n",
    "\n",
    "# best model architecture\n",
    "model_now = build_model(best_config[0])\n",
    "model_now.compile(loss='cross_entropy', optimizer='Adam', learning_rate=best_config[1])\n",
    "\n",
    "input_dim = 784  #  28x28 image \n",
    "for i, layer in enumerate(model_now.layers):\n",
    "    layer.build(input_dim)    # Build layer to initialize nodes\n",
    "    input_dim = layer.units   \n",
    "\n",
    "for layer, layer_data in zip(model_now.layers, weights_data):\n",
    "    for node, weights, bias in zip(layer.nodes, layer_data['weights'], layer_data['biases']):\n",
    "        node.weights = np.array(weights)\n",
    "        node.bias = np.array(bias)\n",
    "    \n",
    "    # batch normalization parameters EKHANE ----------------------------------\n",
    "    if layer.batch_norm is not None:\n",
    "        layer.batch_norm.params.gamma = np.array(layer_data['gamma'])\n",
    "        layer.batch_norm.params.beta = np.array(layer_data['beta'])\n",
    "        layer.batch_norm.running_stats.running_mean = np.array(layer_data['r_mean'])\n",
    "        layer.batch_norm.running_stats.running_var = np.array(layer_data['r_var'])\n",
    "        layer.batch_norm.params.epsilon = layer_data['epsilon']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting labels for the test set\n",
    "predictions = model_now.predict(x_test)\n",
    "test_accuracy = np.mean(predictions == np.argmax(y_test, axis=1))\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
