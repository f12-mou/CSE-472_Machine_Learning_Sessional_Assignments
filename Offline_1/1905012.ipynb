{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_a1: Import the dataset in a notebook environment with python library : “Pandas”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_a2: Show the number of attributes (columns) and number of records (rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the first value denotes number of rows and second value denotes the number of attributes\n",
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the name of the columns\n",
    "dataframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_a3: Show the statistics of the dataset ( column wise mean, standard deviation, max,\n",
    "min etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It shows 8 different statistics of 26 numerical columns\n",
    "dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_a4: Count the number of missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column wise missing value counts\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_a5: Count the number of duplicate values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It shows number of redundant rows\n",
    "dataframe.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which rows are duplicated, the phrase in third bracket returns the indices where duplicates occur\n",
    "dataframe[dataframe.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_b1: If you find any missing values in the dataset ( nan values) replace those data with\n",
    "the column wise mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing any null attributes with the mean of that attribute column\n",
    "dataframe.fillna(dataframe.mean(numeric_only=True), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to validate that we now do not have any Attributes(numerical) null\n",
    "dataframe.isnull().sum()\n",
    "# We will not deal with the string attributes as our target variable is of type string and we have to omit the rows with null values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_b2: If you find any duplicates in the dataset, keep just one copy of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_b3: If any row in the target column (Attrition) is missing, you must drop that row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the row with no target labels\n",
    "dataframe.dropna(subset=['Attrition'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to validate that we have no null values in target attribute\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now replacing the null values of type string with the mode of that column\n",
    "dataframe['Department'].fillna(dataframe['Department'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to validate that we have no null values in the full dataset\n",
    "dataframe.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_3a: You need to split the data into two parts. The “Features” variable will consist of all\n",
    "the columns in the dataset except the target column. And the “Labels” variable\n",
    "will contain only the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = dataframe.drop('Attrition', axis=1)\n",
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels = dataframe['Attrition']\n",
    "Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_d2: For doing that, you need to first convert such columns which are not numeric\n",
    "types, into categorical types. Then you need to perform one hot encoding on that\n",
    "column, which will divide that column into multiple one hot type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "categorical_columns = []\n",
    "for col in Features.columns:\n",
    "\tif (not pd.api.types.is_numeric_dtype(Features[col])):\n",
    "\t\tif (Features[col].nunique() >= 3):\n",
    "\t\t\tcategorical_columns.append(col)\n",
    "\t\telse:\n",
    "\t\t\tencoder = LabelEncoder()\n",
    "\t\t\tdummy_col = Features[col]\n",
    "\t\t\tdummy_col = encoder.fit_transform(dummy_col)\n",
    "\t\t\tdummy_col_df = pd.DataFrame(dummy_col, columns=[col])\n",
    "\t\t\tFeatures[col] = dummy_col_df\n",
    "\t\t\tprint(col)\n",
    "\n",
    "\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each column to categorical type\n",
    "for col in categorical_columns:\n",
    "    Features[col] = Features[col].astype('category')\n",
    "# One-hot encode the data using pandas get_dummies\n",
    "Features = pd.get_dummies(Features, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_col = encoder.fit_transform(Labels)\n",
    "labels_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Labels_df = pd.DataFrame(labels_col, columns=['Attrition'])\n",
    "Labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_e: Scaling of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def scaling(dataframe, preference):\n",
    "\tone_hot_columns=[]\n",
    "\tfor col in dataframe.columns:\n",
    "\t\tfor name in categorical_columns:\n",
    "\t\t\tnow = str(name)+\"_\"\n",
    "\t\t\tif (now in col):\n",
    "\t\t\t\tone_hot_columns.append(col)\n",
    "\t\t\t\t#print(col)\n",
    "\t\t\t\tbreak\n",
    "\t\n",
    "\tother_columns = []\n",
    "\tfor col in dataframe.columns:\n",
    "\t\tif col not in one_hot_columns:\n",
    "\t\t\tother_columns.append(col)\n",
    "\n",
    "\tif preference == 1:\n",
    "\t\tscaler = MinMaxScaler()\n",
    "\t\tfeatures_minmax = scaler.fit_transform(dataframe[other_columns])\n",
    "\t\tfeatures_df = pd.DataFrame(features_minmax, columns=other_columns) # scaled feature dataframe\n",
    "\t\tdataframe[other_columns] = features_df[other_columns]\n",
    "\t\treturn dataframe\n",
    "\telse:\n",
    "\t\tscaler = StandardScaler()\n",
    "\t\tfeatures_minmax = scaler.fit_transform(dataframe[other_columns])\n",
    "\t\tfeatures_df = pd.DataFrame(features_minmax, columns=other_columns) # scaled feature dataframe\n",
    "\t\tdataframe[other_columns] = features_df[other_columns]\n",
    "\t\treturn dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_1 = scaling(Features,1)\n",
    "Features_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_2 = scaling(Features,2)\n",
    "Features_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task_e: Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation analysis of features with target\n",
    "zero_var_columns=[]\n",
    "for col in Features_1.columns:\n",
    "\tvariance = Features_1[col].var()\n",
    "\tif variance == 0:\n",
    "\t\tprint(col)\n",
    "\t\tzero_var_columns.append(col)\n",
    "\n",
    "zero_var_columns\n",
    "target_labels = Labels_df['Attrition']\n",
    "Features_1_cleaned = Features_1.drop(columns=zero_var_columns)\n",
    "correlations = Features_1_cleaned.corrwith(target_labels)\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_correlations = correlations.abs().sort_values(ascending=False)\n",
    "top_20_columns = sorted_correlations.head(20).index\n",
    "top_20_features = Features_1_cleaned[top_20_columns]\n",
    "top_20_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "top_20_features_matrix = top_20_features.corr()\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(top_20_features_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1, fmt='.2f')\n",
    "plt.title('Correlation Heatmap of Top 20 Features')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vals = Labels_df['Attrition'].unique()\n",
    "print(target_vals)\n",
    "class_dict={}\n",
    "for val in target_vals:\n",
    "\tprint(val)\n",
    "\tclass_dict[val]=Features_1_cleaned.loc[Labels_df['Attrition']==val]\n",
    "\n",
    "print(class_dict[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 1D scatter plot for sepal_width with numeric labels\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for col in top_20_columns:\n",
    "\tplt.plot(class_dict[0][col], np.zeros_like(class_dict[0][col]), 'o', label=f'Class {0}')\n",
    "\tplt.plot(class_dict[1][col], np.zeros_like(class_dict[1][col]), 'o', label=f'Class {1}')\n",
    "\tplt.legend()\n",
    "\tplt.xlabel(col)\n",
    "\tplt.title(f'1D Scatter Plot of {col} by Numeric Classes')\n",
    "\tplt.savefig(f\"{col}.png\")\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X = top_20_features\n",
    "y = Labels_df['Attrition']\n",
    "\n",
    "# Step 1: Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=48)\n",
    "# Step 2: Initialize the Logistic Regression classifier\n",
    "clf = LogisticRegression()\n",
    "# Step 3: Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "# Step 4: Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "# Step 5: Evaluate the classifier's performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Logistic Regression classifier: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
