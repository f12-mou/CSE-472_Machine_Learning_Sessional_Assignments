{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import numpy as np\t\t\t\t\t\t\t\t\t\t\t\t# to deal with numpy arrays\n",
    "import pandas as pd\t\t\t\t\t\t\t\t\t\t\t\t# to work with dataframes\n",
    "import math\t\t\t\t\t\t\t\t\t\t\t\t\t\t# if needed\n",
    "import statistics\t\t\t\t\t\t\t\t\t\t\t\t# to get mean and stdev\n",
    "import matplotlib.pyplot as plt\t\t\t\t\t\t\t\t\t# used for plots\n",
    "import seaborn as sns\t\t\t\t\t\t\t\t\t\t\t# for violin curves\n",
    "from sklearn.preprocessing import LabelEncoder\t\t\t\t\t# encoding\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler  # scaling\n",
    "from ucimlrepo import fetch_ucirepo \t\t\t\t\t\t\t# to import 2nd dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function takes W and X, dot product them and pass them via a sigmoid function\n",
    "def hypothesis(train_X, W):\n",
    "\tresult = np.dot(train_X, W.T) \t\t# for 2D arrays, np.dot returns matrix multiplication\n",
    "\tresult = 1 / (1+np.exp(-result)) \t# sigmoid function\n",
    "\treturn result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function converts required columns to Label encoded columns and gets the columns for one-hot encoding\n",
    "def Labeling(Features):\n",
    "\tcategorical_columns = []\n",
    "\tfor col in Features.columns:\n",
    "\t\tif (not pd.api.types.is_numeric_dtype(Features[col])):\n",
    "\t\t\tif (Features[col].nunique() >= 3):\n",
    "\t\t\t\tcategorical_columns.append(col)\n",
    "\t\t\telse:\n",
    "\t\t\t\tencoder = LabelEncoder()\n",
    "\t\t\t\tFeatures[col] = encoder.fit_transform(Features[col])\n",
    "\t\t\t\tprint(col)\n",
    "\t\n",
    "\tprint(categorical_columns)\n",
    "\t# Convert each column to categorical type\n",
    "\tfor col in categorical_columns:\n",
    "\t\tFeatures[col] = Features[col].astype('category')\n",
    "\t# One-hot encode the data using pandas get_dummies\n",
    "\tFeatures = pd.get_dummies(Features, dtype=int)\n",
    "\treturn Features, categorical_columns\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function does scaling based on preference (1: MinMax Scaling, 2: Standard Scaling)\n",
    "def scaling(dataframe, preference, categorical_columns):\n",
    "\tone_hot_columns=[]\n",
    "\tfor col in dataframe.columns:\n",
    "\t\tfor name in categorical_columns: # categorical columns are excluded from scaling due to \n",
    "\t\t\tnow = str(name)+\"_\"\t\t\t # increased efficiency\n",
    "\t\t\tif (now in col):\n",
    "\t\t\t\tone_hot_columns.append(col)\n",
    "\t\t\t\tbreak\n",
    "\t\n",
    "\tother_columns = []\n",
    "\tfor col in dataframe.columns:\n",
    "\t\tif col not in one_hot_columns:\n",
    "\t\t\tother_columns.append(col)\n",
    "\tif preference == 1:\n",
    "\t\tscaler = MinMaxScaler()\n",
    "\t\tdataframe[other_columns] = scaler.fit_transform(dataframe[other_columns])\n",
    "\t\treturn dataframe\n",
    "\telse:\n",
    "\t\tscaler = StandardScaler()\n",
    "\t\tdataframe[other_columns] = scaler.fit_transform(dataframe[other_columns])\n",
    "\t\treturn dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions takes a W vector(LR Model) and Test Data to return accuracy, probabilistic predictions and binary predictions\n",
    "def get_accuracy_results(test_X, test_Y, W):\n",
    "\tresults = hypothesis(test_X, W) # floating point numbers\n",
    "\tpredictions = np.zeros((test_X.shape[0],1))\n",
    "\tfor i in range(test_X.shape[0]):\n",
    "\t\tif results[i] >= 0.5:\t\t# Threashold is taken as 0.5\n",
    "\t\t\tpredictions[i]=1\n",
    "\t\telse:\n",
    "\t\t\tpredictions[i]=0\n",
    "\t\n",
    "\tcorrect_predictions = np.sum(predictions == test_Y)\n",
    "\taccuracy = correct_predictions/len(test_Y)\n",
    "\treturn accuracy, predictions, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross validation, we need to calculate loss for validation set\n",
    "def get_loss(test_Y, Predictions):\n",
    "\tsquared_diff = np.square(test_Y-Predictions)\n",
    "\tmean_squared_loss = np.mean(squared_diff)\n",
    "\treturn mean_squared_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base Class - Logistic Regression Classifier\n",
    "# Parameters:\t\n",
    "#\t\t\t\talpha - Learning rate\n",
    "#\t\t\t\titerations - Number of iterations\n",
    "#\t\t\t\tbatch size - takes as 1000 for Minibatch \n",
    "#\t\t\t\tmodel seed - so that init W is initalized as same as experients\n",
    "\n",
    "def logistic_regression(train_X, train_Y, model_seed, alpha=0.0005, iterations=300, batch_size=1000):\n",
    "\tnumber_of_records, number_of_features = train_X.shape\n",
    "\tnp.random.seed(seed=model_seed)\n",
    "\tinit_W = np.random.rand(1, number_of_features)  # Initializing model weights\n",
    "\n",
    "\n",
    "\tstart = 0\n",
    "\tendIdx = start+batch_size\n",
    "\n",
    "\tfor i in range(iterations):\n",
    "\t\tbatch_X = train_X[start:endIdx]\n",
    "\t\tbatch_Y = train_Y[start:endIdx]\n",
    "\n",
    "\t\tnow = hypothesis(batch_X, init_W) # here we get the probabilities\n",
    "\t\tones_arr = np.ones((endIdx-start,1)) \t\t# Used in formula\n",
    "\n",
    "\t\tfor j in range(0, number_of_features):\n",
    "\t\t\tbatch_X_j = batch_X[:,j].reshape(-1,1)\n",
    "\t\t\tbefore_val = init_W[:,j]\n",
    "\t\t\t\n",
    "\t\t\t#tmp = np.multiply(now,(ones_arr-now))\n",
    "\t\t\ttmp = ones_arr\n",
    "\t\t\there = np.multiply((batch_Y-now), (np.multiply(tmp,(batch_X_j))))\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tinit_W[:,j] = before_val + alpha* here.sum()\n",
    "\n",
    "\n",
    "\t\tstart=endIdx%number_of_records\n",
    "\t\tendIdx=(start+batch_size)\n",
    "\t\tif endIdx >= number_of_records:\n",
    "\t\t\tendIdx = number_of_records\n",
    "\t\t\n",
    "\t\t# norm = np.linalg.norm(init_W)\n",
    "\t\t# if norm != 0:\n",
    "\t\t# \tinit_W = init_W/norm\n",
    "\n",
    "\treturn init_W\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 hyperparameters are tuned via cross validation \n",
    "# alpha - Learning Rate\n",
    "# itr - Number of iteration through which model can fit\n",
    "def cross_validation(train_X, train_Y, validation_X, validation_Y, model_seed):\n",
    "\talpha_list = [0.0001, 0.0005, 0.0007, 0.001, 0.005, 0.007]\n",
    "\titerations_cadidate = [10, 50, 70, 100, 150, 200, 300, 450]\n",
    "\n",
    "\tloss = np.inf\n",
    "\toptimal_alpha = 0.0001\n",
    "\toptimal_itr = 50\n",
    "\toptimal_W = []\n",
    "\n",
    "\tfor alpha in alpha_list:\n",
    "\t\tfor itr in iterations_cadidate:\n",
    "\t\t\tW = logistic_regression(train_X, train_Y, model_seed, alpha, itr, 1000)\n",
    "\t\t\tPredictions_on_validation = hypothesis(validation_X, W)\n",
    "\t\t\tloss_on_validation = get_loss(validation_Y, Predictions_on_validation)\n",
    "\t\t\t#print(\"alpha: \", alpha, \"itr: \", itr, \"loss: \", loss_on_validation)\n",
    "\t\t\t#print(W)\n",
    "\t\t\tif loss_on_validation < loss:\n",
    "\t\t\t\tloss = loss_on_validation\n",
    "\t\t\t\toptimal_alpha = alpha\n",
    "\t\t\t\toptimal_itr = itr\n",
    "\t\t\t\toptimal_W = W\n",
    "\n",
    "\treturn optimal_W, optimal_alpha, optimal_itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my custom dataset splitting. 2nd set is splitted according to ratio\n",
    "def dataset_splitting(dataset_X, dataset_Y, ratio, splitting_seed=90):\n",
    "\ttotal_samples = dataset_X.shape[0]\n",
    "\ttest_size = int(total_samples * ratio)\n",
    "\n",
    "\tall_indices = np.arange(total_samples)\n",
    "\n",
    "\tnp.random.seed(splitting_seed)\n",
    "\tnp.random.shuffle(all_indices)\n",
    "\n",
    "\ttest_indices = all_indices[:test_size]\n",
    "\ttraining_indices = all_indices[test_size:]\n",
    "\n",
    "\ttrain_X = dataset_X.iloc[training_indices].to_numpy()\n",
    "\ttrain_Y = dataset_Y.iloc[training_indices].to_numpy()\n",
    "\n",
    "\ttest_X = dataset_X.iloc[test_indices].to_numpy()\n",
    "\ttest_Y = dataset_Y.iloc[test_indices].to_numpy()\n",
    "\n",
    "\treturn train_X, train_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# to get different performance parameters \n",
    "# for auroc and aupr, predictions are needed rather than binary_predictions\n",
    "\n",
    "def getParameters(truth_labels, binary_predictions, predictions):\n",
    "\ttn, fp, fn, tp = confusion_matrix(truth_labels, binary_predictions).ravel()\n",
    "\t#print(\"tn: \", tn, \"fp: \", fp, \"fn: \", fn, \"tp: \", tp)\n",
    "\ttotal_positive_predictions = tp +fp\n",
    "\tprecision = 0\n",
    "\tif total_positive_predictions != 0:\n",
    "\t\tprecision = tp / (tp + fp)\n",
    "\trecall = tp / (tp + fn)\n",
    "\tf1_score = 0\n",
    "\tif (precision+recall) != 0:\n",
    "\t\tf1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\tspecificity = tn / (tn + fp)\n",
    "\tauroc = roc_auc_score(truth_labels, predictions)\n",
    "\taupr = average_precision_score(truth_labels, predictions)\n",
    "\n",
    "\treturn recall, specificity, precision, f1_score, auroc, aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get different mean values for the 9 LR learners performnace metrics\n",
    "def get_mean_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list):\n",
    "\taccuracy_mean = statistics.mean(accuracy_list)\n",
    "\tsensitivity_mean = statistics.mean(sensitivity_list)\n",
    "\tspecificity_mean = statistics.mean(specificity_list)\n",
    "\tprecision_mean = statistics.mean(precision_list)\n",
    "\tf1_score_mean = statistics.mean(f1_score_list)\n",
    "\tauroc_mean = statistics.mean(auroc_list)\n",
    "\taupr_mean = statistics.mean(aupr_list)\n",
    "\treturn accuracy_mean, sensitivity_mean, specificity_mean, precision_mean, f1_score_mean, auroc_mean, aupr_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get different standard deviation values for the 9 LR learners performnace metrics\n",
    "def get_stdev_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list):\n",
    "\taccuracy_stdev = statistics.stdev(accuracy_list)\n",
    "\tsensitivity_stdev = statistics.stdev(sensitivity_list)\n",
    "\tspecificity_stdev = statistics.stdev(specificity_list)\n",
    "\tprecision_stdev = statistics.stdev(precision_list)\n",
    "\tf1_score_stdev = statistics.stdev(f1_score_list)\n",
    "\tauroc_stdev = statistics.stdev(auroc_list)\n",
    "\taupr_stdev = statistics.stdev(aupr_list)\n",
    "\treturn accuracy_stdev, sensitivity_stdev, specificity_stdev, precision_stdev, f1_score_stdev, auroc_stdev, aupr_stdev\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to draw violin plots using sns library\n",
    "def plot_violins(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list, number):\n",
    "\tdata = {\n",
    "    'Accuracy': accuracy_list,\n",
    "    'Sensitivity': sensitivity_list,\n",
    "    'Specificity': specificity_list,\n",
    "    'Precision': precision_list,\n",
    "    'F1 Score': f1_score_list,\n",
    "    'AUROC': auroc_list,\n",
    "    'AUPR': aupr_list\n",
    "\t}\n",
    "\tplt.figure(figsize=(12, 8), dpi=300)  # High resolution plot\n",
    "\tsns.violinplot(data=list(data.values()), inner=\"quart\",palette=\"Set3\")\n",
    "\tplt.xticks(range(len(data)), list(data.keys()), fontsize=12)\n",
    "\tplt.xlabel(\"Metrics\", fontsize=14)\n",
    "\tplt.ylabel(\"Values\", fontsize=14)\n",
    "\tplt.title(\"Violin Plots for Each Metric\", fontsize=16)\n",
    "\tplt.savefig(f'violin_plot_dataset_{number}.png')\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_list = []\n",
    "main_list.append(['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1 Score', 'AUROC', 'AUPR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- \tPREPROCESSING FOR DATASET-1 -----------------------\n",
    "dataframe = pd.read_csv(\"dataset_1.csv\")\n",
    "dataframe.drop('customerID', axis=1, inplace=True)\n",
    "dataframe['TotalCharges'] = pd.to_numeric(dataframe['TotalCharges'], errors='coerce')\n",
    "dataframe['TotalCharges'].fillna(0)\n",
    "# Replacing any null attributes with the mean of that attribute column\n",
    "dataframe.fillna(dataframe.mean(numeric_only=True), inplace=True)\n",
    "dataframe.drop_duplicates(inplace=True)\n",
    "dataframe.dropna(subset=['Churn'], inplace=True)\n",
    "for column in dataframe.columns:\n",
    "    how_many = dataframe[column].isnull().sum()\n",
    "    if how_many != 0:\n",
    "            dataframe[column]=dataframe[column].fillna(dataframe[column].mode()[0])\n",
    "\n",
    "\n",
    "Features = dataframe.drop('Churn', axis=1)\n",
    "Labels = dataframe['Churn']\n",
    "\n",
    "Features, categorical_columns = Labeling(Features)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels_col = encoder.fit_transform(Labels)\n",
    "Labels_df = pd.DataFrame(labels_col, columns=['Churn'])\n",
    "\n",
    "Features_1 = scaling(Features,1, categorical_columns)\n",
    "# correlation analysis of features with target\n",
    "zero_var_columns=[]\n",
    "for col in Features_1.columns:\n",
    "\tvariance = Features_1[col].var()\n",
    "\tif variance == 0:\n",
    "\t\tzero_var_columns.append(col)\n",
    "\n",
    "target_labels = Labels_df['Churn']\n",
    "Features_1_cleaned = Features_1.drop(columns=zero_var_columns)\n",
    "correlations = Features_1_cleaned.corrwith(target_labels)\n",
    "\n",
    "sorted_correlations = correlations.abs().sort_values(ascending=False)\n",
    "top_20_columns = sorted_correlations.head(20).index\t # 20 features show good amount of correlations\n",
    "X = Features_1_cleaned[top_20_columns]\n",
    "y = Labels_df['Churn']\n",
    "X.to_csv(\"Dataset_X_1.csv\")\n",
    "y.to_csv(\"Dataset_Y_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fetch the adult dataset from the link\n",
    "adult = fetch_ucirepo(id=2) \n",
    "# data (as pandas dataframes) \n",
    "X = adult.data.features \n",
    "y = adult.data.targets \n",
    "df = pd.DataFrame(X)\n",
    "df2 = pd.DataFrame(y)\n",
    "income_list = []\n",
    "for i in range(0, df2.shape[0]):\n",
    "\tif '>50K' in df2.iloc[i,0]:\n",
    "\t\tincome_list.append(1)\n",
    "\telse:\n",
    "\t\tincome_list.append(0)\n",
    "len(income_list)\n",
    "df3 = pd.DataFrame(income_list, columns=['income'])\n",
    "df_combined = pd.concat([df, df3], axis=1)\n",
    "print(df_combined.shape)\n",
    "df_combined.to_csv(\"dataset_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Dataset-2 PREPROCESSING ----------------------\n",
    "dataframe = pd.read_csv(\"dataset_2.csv\").iloc[:, 1:]\n",
    "dataframe.drop_duplicates(inplace=True)\n",
    "dataframe.dropna(subset=['income'], inplace=True)\n",
    "for column in dataframe.columns:\n",
    "    how_many = dataframe[column].isnull().sum()\n",
    "    if how_many != 0:\n",
    "            dataframe[column]=dataframe[column].fillna(dataframe[column].mode()[0])\n",
    "\n",
    "dataframe.isnull().sum()\n",
    "dataframe['workclass'].fillna(dataframe['workclass'].mode()[0])\n",
    "Features = dataframe.drop('income', axis=1)\n",
    "Labels = dataframe['income']\n",
    "Features, categorical_columns = Labeling(Features)\n",
    "labels_col = encoder.fit_transform(Labels)\n",
    "Labels_df = pd.DataFrame(labels_col, columns=['income'])\n",
    "\n",
    "Features_1 = scaling(Features,1, categorical_columns)\n",
    "# correlation analysis of features with target\n",
    "zero_var_columns=[]\n",
    "for col in Features_1.columns:\n",
    "\tvariance = Features_1[col].var()\n",
    "\tif variance == 0:\n",
    "\t\tprint(col)\n",
    "\t\tzero_var_columns.append(col)\n",
    "\n",
    "\n",
    "zero_var_columns\n",
    "target_labels = Labels_df['income']\n",
    "Features_1_cleaned = Features_1.drop(columns=zero_var_columns)\n",
    "correlations = Features_1_cleaned.corrwith(target_labels)\n",
    "\n",
    "sorted_correlations = correlations.abs().sort_values(ascending=False)\n",
    "top_20_columns = sorted_correlations.head(20).index\n",
    "X = Features_1_cleaned[top_20_columns]\n",
    "y = Labels_df['income']\n",
    "\n",
    "X.to_csv(\"Dataset_X_2.csv\")\n",
    "y.to_csv(\"Dataset_Y_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"creditcard.csv\")\n",
    "dataframe.drop_duplicates(inplace=True)\n",
    "dataframe.dropna(subset=['Class'], inplace=True)\n",
    "\n",
    "Features = dataframe.drop('Class', axis=1)\n",
    "Labels = dataframe['Class']\n",
    "\n",
    "Features, categorical_columns = Labeling(Features)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels_col = encoder.fit_transform(Labels)\n",
    "Labels_df = pd.DataFrame(labels_col, columns=['Class'])\n",
    "\n",
    "Features_1 = scaling(Features,1,categorical_columns)\n",
    "# correlation analysis of features with target\n",
    "zero_var_columns=[]\n",
    "for col in Features_1.columns:\n",
    "\tvariance = Features_1[col].var()\n",
    "\tif variance == 0:\n",
    "\t\tzero_var_columns.append(col)\n",
    "\n",
    "target_labels = Labels_df['Class']\n",
    "Features_1_cleaned = Features_1.drop(columns=zero_var_columns)\n",
    "correlations = Features_1_cleaned.corrwith(target_labels)\n",
    "\n",
    "sorted_correlations = correlations.abs().sort_values(ascending=False)\n",
    "top_20_columns = sorted_correlations.head(20).index\n",
    "X = Features_1_cleaned[top_20_columns]\n",
    "y = Labels_df['Class']\n",
    "\n",
    "X.to_csv(\"Dataset_X_3.csv\")\n",
    "y.to_csv(\"Dataset_Y_3.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset_X = pd.read_csv(\"Dataset_X_1.csv\", skiprows = 1).iloc[: , 1:]\n",
    "dataset_Y = pd.read_csv(\"Dataset_Y_1.csv\", skiprows = 1).iloc[: , 1:]\n",
    "dummy_ones_column = np.ones((temp_dataset_X.shape[0],1)) # this creates a n*1 matrix of 1's\n",
    "dataset_X = np.hstack((dummy_ones_column, temp_dataset_X)) # (n, m+1)\n",
    "dataset_X=pd.DataFrame(dataset_X)\n",
    "init_train_X_1, init_train_Y_1, test_X_1, test_Y_1  = dataset_splitting(dataset_X, dataset_Y, 0.2, 96)\n",
    "train_X_1, train_Y_1, validation_X_1, validation_Y_1 = dataset_splitting(pd.DataFrame(init_train_X_1), pd.DataFrame(init_train_Y_1), 0.2, 85)\n",
    "W_1, alpha_1, itr_1 = cross_validation(train_X_1, train_Y_1, validation_X_1, validation_Y_1, 48)\n",
    "accu, binary_predictions, predictions= get_accuracy_results(test_X_1, test_Y_1, W_1)\n",
    "print(\"accuracy: \", accu, \"optimal_alpha: \", alpha_1, \"optimal_iteration_num: \", itr_1)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(test_Y_1, binary_predictions, predictions)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accu, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- 9 LR MODELS FOR BAGGING -----------------------\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "all_indices = np.arange(dataset_X.shape[0])\n",
    "training_size = (int)(0.80 * dataset_X.shape[0])\t\t# 80% training set\n",
    "np.random.seed(48)\n",
    "training_indices = np.random.choice(all_indices, size=training_size, replace=False)\n",
    "test_indices=[]\n",
    "\n",
    "for i in all_indices:\n",
    "\tif i not in training_indices:\n",
    "\t\ttest_indices.append(i)\n",
    "\n",
    "test_labels = dataset_Y.iloc[test_indices].to_numpy()\n",
    "W_list = []\n",
    "validation_X_list =[]\n",
    "validation_Y_list =[]\n",
    "seeds = [27,20,30,74,56,76,56,78,90,10,23,49,56]\n",
    "splitting_seeds = [96, 98, 32, 54, 10, 15, 28, 95, 12, 39, 48, 68, 71]\n",
    "\n",
    "Testset_X = dataset_X.iloc[test_indices]\n",
    "Testset_Y = dataset_Y.iloc[test_indices]\n",
    "\n",
    "for i in range(0,9):\n",
    "\t\n",
    "\tnp.random.seed(seeds[i])\n",
    "\trandom_indices = np.random.choice(training_indices, size=(training_size), replace=True)\n",
    "\n",
    "\t# Training + Validation sets ----------------\n",
    "\tdataset_X_i = dataset_X.iloc[random_indices]\n",
    "\tdataset_Y_i = dataset_Y.iloc[random_indices]\n",
    "\t# Training + Validation sets ----------------\n",
    "\n",
    "\ttrain_X, train_Y, validation_X, validation_Y = dataset_splitting(dataset_X_i, dataset_Y_i, 0.2, splitting_seeds[i])\n",
    "\tvalidation_X_list.append(validation_X)\n",
    "\tvalidation_Y_list.append(validation_Y)\n",
    "\tW, alpha, itr = cross_validation(train_X, train_Y, validation_X, validation_Y, seeds[i])\n",
    "\tW_list.append(W) \n",
    "\n",
    "\n",
    "# ----------- We have 9 different LR models (W's) -------------\n",
    "Binary_Prediction_list =[]\n",
    "Prediction_list =[]\n",
    "accuracy_list = []\n",
    "sensitivity_list=[]\n",
    "specificity_list=[]\n",
    "precision_list=[]\n",
    "f1_score_list=[]\n",
    "auroc_list=[]\n",
    "aupr_list=[]\n",
    "\n",
    "final_predictions = np.zeros((Testset_Y.shape[0],1))\n",
    "for i in range(0, 9):\n",
    "\taccuracy, binary_predictions, predictions = get_accuracy_results(Testset_X.to_numpy(), Testset_Y.to_numpy(), W_list[i])\n",
    "\t\n",
    "\tprint(accuracy)\n",
    "\n",
    "\tBinary_Prediction_list.append(binary_predictions)\n",
    "\tPrediction_list.append(predictions)\n",
    "\n",
    "\tfinal_predictions=final_predictions+binary_predictions\n",
    "\n",
    "\taccuracy_list.append(accuracy)\n",
    "\tsensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(Testset_Y, binary_predictions, predictions)\n",
    "\tprint(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "\tsensitivity_list.append(sensitivity)\n",
    "\tspecificity_list.append(specificity)\n",
    "\tprecision_list.append(precision)\n",
    "\tf1_score_list.append(f1_score)\n",
    "\tauroc_list.append(auroc)\n",
    "\taupr_list.append(aupr)\n",
    "\t\n",
    "\n",
    "\n",
    "for i in range(len(final_predictions)):\n",
    "\tif final_predictions[i] >=5:\n",
    "\t\tfinal_predictions[i]=1\n",
    "\telse:\n",
    "\t\tfinal_predictions[i]=0\n",
    "\n",
    "\n",
    "correct_predictions = np.sum(final_predictions == Testset_Y.to_numpy())\n",
    "accuracy = correct_predictions / Testset_Y.shape[0]\n",
    "sensitivity = recall_score(Testset_Y, final_predictions)\n",
    "print(\"Final Accuracy:\", accuracy)\n",
    "print(\"final Sensitivity: \", sensitivity)\n",
    "print(sensitivity_list)\n",
    "\n",
    "stacked_predictions_1 = np.hstack(Prediction_list)\n",
    "mean_predictions = np.mean(stacked_predictions_1, axis=1)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(Testset_Y, final_predictions, mean_predictions)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accuracy, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_mean,sens_mean, spec_mean, pre_mean, f1_mean, auroc_mean, aupr_mean = get_mean_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list)\n",
    "accu_std,sens_std, spec_std, pre_std, f1_std, auroc_std, aupr_std = get_stdev_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list)\n",
    "print(\"Mean Scores for Bagging:\")\n",
    "print(\"Accuracy: \", accu_mean, \"Sensitivity: \", sens_mean, \"Specificity: \", spec_mean, \"Precision: \", pre_mean, \"F1_score: \", f1_score, \"AUROC: \", auroc_mean, \"AUPR: \", aupr_mean)\n",
    "print(\"Standard Deviations of Metrics for Bagging:\")\n",
    "print(\"Accuracy: \", accu_std, \"Sensitivity: \", sens_std, \"Specificity: \", spec_std, \"Precision: \", pre_std, \"F1_score: \", f1_std, \"AUROC: \", auroc_std, \"AUPR: \", aupr_std)\n",
    "main_list.append([accu_mean,sens_mean, spec_mean, pre_mean, f1_mean, auroc_mean, aupr_mean])\n",
    "main_list.append([accu_std,sens_std, spec_std, pre_std, f1_std, auroc_std, aupr_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violins(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the validation data for input in stacking\n",
    "df_X = pd.DataFrame(np.vstack(validation_X_list))\n",
    "df_Y = pd.DataFrame(np.vstack(validation_Y_list),columns=[\"Y\"])\n",
    "merged_dataframe = pd.concat([df_X, df_Y], axis=1)\n",
    "merged_dataframe = merged_dataframe.drop_duplicates()\n",
    "final_X = merged_dataframe.drop(columns=[\"Y\"]).to_numpy() \n",
    "final_Y = merged_dataframe[\"Y\"].to_numpy().reshape(-1,1)\n",
    "print(\"Final X shape:\", final_X.shape)\n",
    "print(\"Final Y shape:\", final_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stacking_X = pd.DataFrame(final_X)\n",
    "\n",
    "for i in range(0,9):\n",
    "\taccuracy, predictions,_ = get_accuracy_results(final_X, final_Y, W_list[i])\n",
    "\tdf_temp = pd.DataFrame(predictions)\n",
    "\tdataset_stacking_X = pd.concat([dataset_stacking_X, df_temp], axis=1)\n",
    "\n",
    "dataset_stacking_Y = pd.DataFrame(final_Y)\n",
    "print(dataset_stacking_X.shape)\n",
    "print(dataset_stacking_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_train_X_stacking_1, init_train_Y_stacking_1, test_X_stacking_1, test_Y_stacking_1  = dataset_splitting(dataset_stacking_X, dataset_stacking_Y, 0.2, 96)\n",
    "train_X_stacking_1, train_Y_stacking_1, validation_X_stacking_1, validation_Y_stacking_1 = dataset_splitting(pd.DataFrame(init_train_X_stacking_1), pd.DataFrame(init_train_Y_stacking_1), 0.2, 34)\n",
    "W_stacking_1, alpha_stacking_1, itr_stacking_1 = cross_validation(train_X_stacking_1, train_Y_stacking_1, validation_X_stacking_1, validation_Y_stacking_1, 32)\n",
    "accu_stacking_1, binary_predictions_stacking_1, predictions_stacking_1 = get_accuracy_results(test_X_stacking_1, test_Y_stacking_1, W_stacking_1)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(test_Y_stacking_1, binary_predictions_stacking_1, predictions_stacking_1)\n",
    "print(\"accuracy: \", accu_stacking_1, \"optimal_alpha: \", alpha_stacking_1, \"optimal_iteration_num: \", itr_stacking_1)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accu_stacking_1, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset_X = pd.read_csv(\"Dataset_X_2.csv\", skiprows = 1).iloc[: , 1:]\n",
    "dataset_Y = pd.read_csv(\"Dataset_Y_2.csv\", skiprows = 1).iloc[: , 1:]\n",
    "dummy_ones_column = np.ones((temp_dataset_X.shape[0],1)) # this creates a n*1 matrix of 1's\n",
    "dataset_X = np.hstack((dummy_ones_column, temp_dataset_X)) # (n, m+1)\n",
    "dataset_X=pd.DataFrame(dataset_X)\n",
    "init_train_X_1, init_train_Y_1, test_X_1, test_Y_1  = dataset_splitting(dataset_X, dataset_Y, 0.2, 96)\n",
    "train_X_1, train_Y_1, validation_X_1, validation_Y_1 = dataset_splitting(pd.DataFrame(init_train_X_1), pd.DataFrame(init_train_Y_1), 0.2, 85)\n",
    "W_1, alpha_1, itr_1 = cross_validation(train_X_1, train_Y_1, validation_X_1, validation_Y_1, 48)\n",
    "accu, binary_predictions, predictions= get_accuracy_results(test_X_1, test_Y_1, W_1)\n",
    "print(\"accuracy: \", accu, \"optimal_alpha: \", alpha_1, \"optimal_iteration_num: \", itr_1)\n",
    "print(dataset_Y.shape)\n",
    "print(dataset_X.shape)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(test_Y_1, binary_predictions, predictions)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accu, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- 9 LR MODELS FOR BAGGING -----------------------\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "all_indices = np.arange(dataset_X.shape[0])\n",
    "training_size = (int)(0.80 * dataset_X.shape[0])\t\t# 80% training set\n",
    "np.random.seed(48)\n",
    "training_indices = np.random.choice(all_indices, size=training_size, replace=False)\n",
    "test_indices=[]\n",
    "\n",
    "for i in all_indices:\n",
    "\tif i not in training_indices:\n",
    "\t\ttest_indices.append(i)\n",
    "\n",
    "test_labels = dataset_Y.iloc[test_indices].to_numpy()\n",
    "W_list = []\n",
    "validation_X_list =[]\n",
    "validation_Y_list =[]\n",
    "seeds = [27,20,30,74,56,76,56,78,90,10,23,49,56]\n",
    "splitting_seeds = [96, 98, 32, 54, 10, 15, 28, 95, 12, 39, 48, 68, 71]\n",
    "\n",
    "Testset_X = dataset_X.iloc[test_indices]\n",
    "Testset_Y = dataset_Y.iloc[test_indices]\n",
    "\n",
    "for i in range(0,9):\n",
    "\t\n",
    "\tnp.random.seed(seeds[i])\n",
    "\trandom_indices = np.random.choice(training_indices, size=(training_size), replace=True)\n",
    "\n",
    "\t# Training + Validation sets ----------------\n",
    "\tdataset_X_i = dataset_X.iloc[random_indices]\n",
    "\tdataset_Y_i = dataset_Y.iloc[random_indices]\n",
    "\t# Training + Validation sets ----------------\n",
    "\n",
    "\ttrain_X, train_Y, validation_X, validation_Y = dataset_splitting(dataset_X_i, dataset_Y_i, 0.2, splitting_seeds[i])\n",
    "\tvalidation_X_list.append(validation_X)\n",
    "\tvalidation_Y_list.append(validation_Y)\n",
    "\tW, alpha, itr = cross_validation(train_X, train_Y, validation_X, validation_Y, seeds[i])\n",
    "\tW_list.append(W) \n",
    "\n",
    "\n",
    "# ----------- We have 9 different LR models (W's) -------------\n",
    "Binary_Prediction_list =[]\n",
    "Prediction_list =[]\n",
    "accuracy_list = []\n",
    "sensitivity_list=[]\n",
    "specificity_list=[]\n",
    "precision_list=[]\n",
    "f1_score_list=[]\n",
    "auroc_list=[]\n",
    "aupr_list=[]\n",
    "\n",
    "final_predictions = np.zeros((Testset_Y.shape[0],1))\n",
    "for i in range(0, 9):\n",
    "\taccuracy, binary_predictions, predictions = get_accuracy_results(Testset_X.to_numpy(), Testset_Y.to_numpy(), W_list[i])\n",
    "\t\n",
    "\tprint(accuracy)\n",
    "\n",
    "\tBinary_Prediction_list.append(binary_predictions)\n",
    "\tPrediction_list.append(predictions)\n",
    "\n",
    "\tfinal_predictions=final_predictions+binary_predictions\n",
    "\n",
    "\taccuracy_list.append(accuracy)\n",
    "\tsensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(Testset_Y, binary_predictions, predictions)\n",
    "\tprint(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "\tsensitivity_list.append(sensitivity)\n",
    "\tspecificity_list.append(specificity)\n",
    "\tprecision_list.append(precision)\n",
    "\tf1_score_list.append(f1_score)\n",
    "\tauroc_list.append(auroc)\n",
    "\taupr_list.append(aupr)\n",
    "\t\n",
    "\n",
    "\n",
    "for i in range(len(final_predictions)):\n",
    "\tif final_predictions[i] >=5:\n",
    "\t\tfinal_predictions[i]=1\n",
    "\telse:\n",
    "\t\tfinal_predictions[i]=0\n",
    "\n",
    "\n",
    "correct_predictions = np.sum(final_predictions == Testset_Y.to_numpy())\n",
    "accuracy = correct_predictions / Testset_Y.shape[0]\n",
    "sensitivity = recall_score(Testset_Y, final_predictions)\n",
    "print(\"Final Accuracy:\", accuracy)\n",
    "print(\"final Sensitivity: \", sensitivity)\n",
    "print(sensitivity_list)\n",
    "\n",
    "stacked_predictions_1 = np.hstack(Prediction_list)\n",
    "mean_predictions = np.mean(stacked_predictions_1, axis=1)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(Testset_Y, final_predictions, mean_predictions)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accuracy, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_mean,sens_mean, spec_mean, pre_mean, f1_mean, auroc_mean, aupr_mean = get_mean_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list)\n",
    "accu_std,sens_std, spec_std, pre_std, f1_std, auroc_std, aupr_std = get_stdev_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list)\n",
    "print(\"Dataset-2 Mean Scores for Bagging:\")\n",
    "print(\"Accuracy: \", accu_mean, \"Sensitivity: \", sens_mean, \"Specificity: \", spec_mean, \"Precision: \", pre_mean, \"F1_score: \", f1_score, \"AUROC: \", auroc_mean, \"AUPR: \", aupr_mean)\n",
    "print(\"Dataset-2 Standard Deviations of Metrics for Bagging:\")\n",
    "print(\"Accuracy: \", accu_std, \"Sensitivity: \", sens_std, \"Specificity: \", spec_std, \"Precision: \", pre_std, \"F1_score: \", f1_std, \"AUROC: \", auroc_std, \"AUPR: \", aupr_std)\n",
    "main_list.append([accu_mean,sens_mean, spec_mean, pre_mean, f1_mean, auroc_mean, aupr_mean])\n",
    "main_list.append([accu_std,sens_std, spec_std, pre_std, f1_std, auroc_std, aupr_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violins(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the validation data for input in stacking\n",
    "df_X = pd.DataFrame(np.vstack(validation_X_list))\n",
    "df_Y = pd.DataFrame(np.vstack(validation_Y_list),columns=[\"Y\"])\n",
    "merged_dataframe = pd.concat([df_X, df_Y], axis=1)\n",
    "merged_dataframe = merged_dataframe.drop_duplicates()\n",
    "final_X = merged_dataframe.drop(columns=[\"Y\"]).to_numpy() \n",
    "final_Y = merged_dataframe[\"Y\"].to_numpy().reshape(-1,1)\n",
    "print(\"Final X shape:\", final_X.shape)\n",
    "print(\"Final Y shape:\", final_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stacking_X = pd.DataFrame(final_X)\n",
    "\n",
    "for i in range(0,9):\n",
    "\taccuracy, predictions,_ = get_accuracy_results(final_X, final_Y, W_list[i])\n",
    "\tdf_temp = pd.DataFrame(predictions)\n",
    "\tdataset_stacking_X = pd.concat([dataset_stacking_X, df_temp], axis=1)\n",
    "\n",
    "dataset_stacking_Y = pd.DataFrame(final_Y)\n",
    "print(dataset_stacking_X.shape)\n",
    "print(dataset_stacking_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_train_X_stacking_2, init_train_Y_stacking_2, test_X_stacking_2, test_Y_stacking_2  = dataset_splitting(dataset_stacking_X, dataset_stacking_Y, 0.2, 90)\n",
    "train_X_stacking_2, train_Y_stacking_2, validation_X_stacking_2, validation_Y_stacking_2 = dataset_splitting(pd.DataFrame(init_train_X_stacking_2), pd.DataFrame(init_train_Y_stacking_2), 0.2, 39)\n",
    "W_stacking_2, alpha_stacking_2, itr_stacking_2 = cross_validation(train_X_stacking_2, train_Y_stacking_2, validation_X_stacking_2, validation_Y_stacking_2, 48)\n",
    "accu_stacking_2, binary_predictions_stacking_2, predictions_stacking_2 = get_accuracy_results(test_X_stacking_2, test_Y_stacking_2, W_stacking_2)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(test_Y_stacking_2, binary_predictions_stacking_2, predictions_stacking_2)\n",
    "print(\"accuracy: \", accu_stacking_2, \"optimal_alpha: \", alpha_stacking_2, \"optimal_iteration_num: \", itr_stacking_2)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accu_stacking_2, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataset_X = pd.read_csv(\"Dataset_X_3.csv\", skiprows = 1).iloc[: , 1:]\n",
    "dataset_Y = pd.read_csv(\"Dataset_Y_3.csv\", skiprows = 1).iloc[: , 1:]\n",
    "dummy_ones_column = np.ones((temp_dataset_X.shape[0],1)) # this creates a n*1 matrix of 1's\n",
    "dataset_X = np.hstack((dummy_ones_column, temp_dataset_X)) # (n, m+1)\n",
    "dataset_X=pd.DataFrame(dataset_X)\n",
    "init_train_X_1, init_train_Y_1, test_X_1, test_Y_1  = dataset_splitting(dataset_X, dataset_Y, 0.2, 96)\n",
    "train_X_1, train_Y_1, validation_X_1, validation_Y_1 = dataset_splitting(pd.DataFrame(init_train_X_1), pd.DataFrame(init_train_Y_1), 0.2, 85)\n",
    "W_1, alpha_1, itr_1 = cross_validation(train_X_1, train_Y_1, validation_X_1, validation_Y_1, 48)\n",
    "accu, binary_predictions, predictions= get_accuracy_results(test_X_1, test_Y_1, W_1)\n",
    "print(\"accuracy: \", accu, \"optimal_alpha: \", alpha_1, \"optimal_iteration_num: \", itr_1)\n",
    "print(dataset_Y.shape)\n",
    "print(dataset_X.shape)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(test_Y_1, binary_predictions, predictions)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accu, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------- 9 LR MODELS FOR BAGGING -----------------------\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "all_indices = np.arange(dataset_X.shape[0])\n",
    "training_size = (int)(0.80 * dataset_X.shape[0])\t\t# 80% training set\n",
    "np.random.seed(50)\n",
    "training_indices = np.random.choice(all_indices, size=training_size, replace=False)\n",
    "test_indices=[]\n",
    "\n",
    "for i in all_indices:\n",
    "\tif i not in training_indices:\n",
    "\t\ttest_indices.append(i)\n",
    "\n",
    "test_labels = dataset_Y.iloc[test_indices].to_numpy()\n",
    "\n",
    "W_list = []\n",
    "validation_X_list =[]\n",
    "validation_Y_list =[]\n",
    "seeds = [27,20,30,74,56,76,56,78,90,10,23,49,56]\n",
    "splitting_seeds = [96, 98, 32, 54, 10, 15, 28, 95, 12, 39, 48, 68, 71]\n",
    "\n",
    "Testset_X = dataset_X.iloc[test_indices]\n",
    "Testset_Y = dataset_Y.iloc[test_indices]\n",
    "\n",
    "\n",
    "print(\"TestSet_X shape:\")\n",
    "print(Testset_X.shape)\n",
    "\n",
    "\n",
    "for i in range(0,9):\n",
    "\t\n",
    "\tnp.random.seed(seeds[i])\n",
    "\trandom_indices = np.random.choice(training_indices, size=(training_size), replace=True)\n",
    "\n",
    "\t# Training + Validation sets ----------------\n",
    "\tdataset_X_i = dataset_X.iloc[random_indices]\n",
    "\tdataset_Y_i = dataset_Y.iloc[random_indices]\n",
    "\t# Training + Validation sets ----------------\n",
    "\n",
    "\ttrain_X, train_Y, validation_X, validation_Y = dataset_splitting(dataset_X_i, dataset_Y_i, 0.2, splitting_seeds[i])\n",
    "\tvalidation_X_list.append(validation_X)\n",
    "\tvalidation_Y_list.append(validation_Y)\n",
    "\tW, alpha, itr = cross_validation(train_X, train_Y, validation_X, validation_Y, seeds[i])\n",
    "\tW_list.append(W) \n",
    "\n",
    "\n",
    "# ----------- We have 9 different LR models (W's) -------------\n",
    "Binary_Prediction_list =[]\n",
    "Prediction_list =[]\n",
    "accuracy_list = []\n",
    "sensitivity_list=[]\n",
    "specificity_list=[]\n",
    "precision_list=[]\n",
    "f1_score_list=[]\n",
    "auroc_list=[]\n",
    "aupr_list=[]\n",
    "\n",
    "final_predictions = np.zeros((Testset_Y.shape[0],1))\n",
    "for i in range(0, 9):\n",
    "\taccuracy, binary_predictions, predictions = get_accuracy_results(Testset_X.to_numpy(), Testset_Y.to_numpy(), W_list[i])\n",
    "\t\n",
    "\tprint(accuracy)\n",
    "\n",
    "\tBinary_Prediction_list.append(binary_predictions)\n",
    "\tPrediction_list.append(predictions)\n",
    "\n",
    "\tfinal_predictions=final_predictions+binary_predictions\n",
    "\n",
    "\taccuracy_list.append(accuracy)\n",
    "\tsensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(Testset_Y, binary_predictions, predictions)\n",
    "\tprint(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "\tsensitivity_list.append(sensitivity)\n",
    "\tspecificity_list.append(specificity)\n",
    "\tprecision_list.append(precision)\n",
    "\tf1_score_list.append(f1_score)\n",
    "\tauroc_list.append(auroc)\n",
    "\taupr_list.append(aupr)\n",
    "\t\n",
    "\n",
    "\n",
    "for i in range(len(final_predictions)):\n",
    "\tif final_predictions[i] >=5:\n",
    "\t\tfinal_predictions[i]=1\n",
    "\telse:\n",
    "\t\tfinal_predictions[i]=0\n",
    "\n",
    "\n",
    "correct_predictions = np.sum(final_predictions == Testset_Y.to_numpy())\n",
    "accuracy = correct_predictions / Testset_Y.shape[0]\n",
    "sensitivity = recall_score(Testset_Y, final_predictions)\n",
    "print(\"Final Accuracy:\", accuracy)\n",
    "print(\"final Sensitivity: \", sensitivity)\n",
    "print(sensitivity_list)\n",
    "\n",
    "stacked_predictions_1 = np.hstack(Prediction_list)\n",
    "mean_predictions = np.mean(stacked_predictions_1, axis=1)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(Testset_Y, final_predictions, mean_predictions)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accuracy, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_mean,sens_mean, spec_mean, pre_mean, f1_mean, auroc_mean, aupr_mean = get_mean_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list)\n",
    "accu_std,sens_std, spec_std, pre_std, f1_std, auroc_std, aupr_std = get_stdev_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list)\n",
    "print(\"Dataset-3 Mean Scores for Bagging:\")\n",
    "print(\"Accuracy: \", accu_mean, \"Sensitivity: \", sens_mean, \"Specificity: \", spec_mean, \"Precision: \", pre_mean, \"F1_score: \", f1_score, \"AUROC: \", auroc_mean, \"AUPR: \", aupr_mean)\n",
    "print(\"Dataset-3 Standard Deviations of Metrics for Bagging:\")\n",
    "print(\"Accuracy: \", accu_std, \"Sensitivity: \", sens_std, \"Specificity: \", spec_std, \"Precision: \", pre_std, \"F1_score: \", f1_std, \"AUROC: \", auroc_std, \"AUPR: \", aupr_std)\n",
    "main_list.append([accu_mean,sens_mean, spec_mean, pre_mean, f1_mean, auroc_mean, aupr_mean])\n",
    "main_list.append([accu_std,sens_std, spec_std, pre_std, f1_std, auroc_std, aupr_std])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violins(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the validation data for input in stacking\n",
    "df_X = pd.DataFrame(np.vstack(validation_X_list))\n",
    "df_Y = pd.DataFrame(np.vstack(validation_Y_list),columns=[\"Y\"])\n",
    "merged_dataframe = pd.concat([df_X, df_Y], axis=1)\n",
    "merged_dataframe = merged_dataframe.drop_duplicates()\n",
    "final_X = merged_dataframe.drop(columns=[\"Y\"]).to_numpy() \n",
    "final_Y = merged_dataframe[\"Y\"].to_numpy().reshape(-1,1)\n",
    "print(\"Final X shape:\", final_X.shape)\n",
    "print(\"Final Y shape:\", final_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stacking_X = pd.DataFrame(final_X)\n",
    "\n",
    "for i in range(0,9):\n",
    "\taccuracy, predictions,_ = get_accuracy_results(final_X, final_Y, W_list[i])\n",
    "\tdf_temp = pd.DataFrame(predictions)\n",
    "\tdataset_stacking_X = pd.concat([dataset_stacking_X, df_temp], axis=1)\n",
    "\n",
    "dataset_stacking_Y = pd.DataFrame(final_Y)\n",
    "print(dataset_stacking_X.shape)\n",
    "print(dataset_stacking_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_train_X_stacking_2, init_train_Y_stacking_2, test_X_stacking_2, test_Y_stacking_2  = dataset_splitting(dataset_stacking_X, dataset_stacking_Y, 0.2, 90)\n",
    "train_X_stacking_2, train_Y_stacking_2, validation_X_stacking_2, validation_Y_stacking_2 = dataset_splitting(pd.DataFrame(init_train_X_stacking_2), pd.DataFrame(init_train_Y_stacking_2), 0.2, 39)\n",
    "W_stacking_2, alpha_stacking_2, itr_stacking_2 = cross_validation(train_X_stacking_2, train_Y_stacking_2, validation_X_stacking_2, validation_Y_stacking_2, 48)\n",
    "accu_stacking_3, binary_predictions_stacking_2, predictions_stacking_2 = get_accuracy_results(test_X_stacking_2, test_Y_stacking_2, W_stacking_2)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(test_Y_stacking_2, binary_predictions_stacking_2, predictions_stacking_2)\n",
    "print(\"accuracy: \", accu_stacking_3, \"optimal_alpha: \", alpha_stacking_2, \"optimal_iteration_num: \", itr_stacking_2)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accu_stacking_3, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_where_one = dataset_Y.index[dataset_Y.iloc[:,0] == 1].tolist()\n",
    "\n",
    "# Print the indices\n",
    "print(len(indices_where_one))\n",
    "other_indices=[]\n",
    "for i in range(dataset_X.shape[0]):\n",
    "\tif i not in indices_where_one:\n",
    "\t\tother_indices.append(i)\n",
    "\n",
    "training_size = (int)(0.8*20000) \t# 80% training set\n",
    "np.random.seed(50)\n",
    "all_indices = np.random.choice(other_indices, size=20000, replace=False)\n",
    "all_indices = np.append(all_indices, indices_where_one)\n",
    "\n",
    "training_indices = np.random.choice(all_indices, size=training_size, replace=False)\n",
    "test_indices=[]\n",
    "\n",
    "for i in all_indices:\n",
    "\tif i not in training_indices:\n",
    "\t\ttest_indices.append(i)\n",
    "\n",
    "test_labels = dataset_Y.iloc[test_indices].to_numpy()\n",
    "\n",
    "W_list = []\n",
    "validation_X_list =[]\n",
    "validation_Y_list =[]\n",
    "seeds = [27,20,30,74,56,76,56,78,90,10,23,49,56]\n",
    "splitting_seeds = [96, 98, 32, 54, 10, 15, 28, 95, 12, 39, 48, 68, 71]\n",
    "\n",
    "Testset_X = dataset_X.iloc[test_indices]\n",
    "Testset_Y = dataset_Y.iloc[test_indices]\n",
    "\n",
    "\n",
    "print(\"TestSet_X shape:\")\n",
    "print(Testset_X.shape)\n",
    "\n",
    "\n",
    "for i in range(0,9):\n",
    "\t\n",
    "\tnp.random.seed(seeds[i])\n",
    "\trandom_indices = np.random.choice(training_indices, size=(training_size), replace=True)\n",
    "\n",
    "\t# Training + Validation sets ----------------\n",
    "\tdataset_X_i = dataset_X.iloc[random_indices]\n",
    "\tdataset_Y_i = dataset_Y.iloc[random_indices]\n",
    "\t# Training + Validation sets ----------------\n",
    "\n",
    "\ttrain_X, train_Y, validation_X, validation_Y = dataset_splitting(dataset_X_i, dataset_Y_i, 0.2, splitting_seeds[i])\n",
    "\tvalidation_X_list.append(validation_X)\n",
    "\tvalidation_Y_list.append(validation_Y)\n",
    "\tW, alpha, itr = cross_validation(train_X, train_Y, validation_X, validation_Y, seeds[i])\n",
    "\tW_list.append(W) \n",
    "\n",
    "\n",
    "# ----------- We have 9 different LR models (W's) -------------\n",
    "Binary_Prediction_list =[]\n",
    "Prediction_list =[]\n",
    "accuracy_list = []\n",
    "sensitivity_list=[]\n",
    "specificity_list=[]\n",
    "precision_list=[]\n",
    "f1_score_list=[]\n",
    "auroc_list=[]\n",
    "aupr_list=[]\n",
    "\n",
    "final_predictions = np.zeros((Testset_Y.shape[0],1))\n",
    "for i in range(0, 9):\n",
    "\taccuracy, binary_predictions, predictions = get_accuracy_results(Testset_X.to_numpy(), Testset_Y.to_numpy(), W_list[i])\n",
    "\t\n",
    "\tprint(accuracy)\n",
    "\n",
    "\tBinary_Prediction_list.append(binary_predictions)\n",
    "\tPrediction_list.append(predictions)\n",
    "\n",
    "\tfinal_predictions=final_predictions+binary_predictions\n",
    "\n",
    "\taccuracy_list.append(accuracy)\n",
    "\tsensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(Testset_Y, binary_predictions, predictions)\n",
    "\tprint(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "\tsensitivity_list.append(sensitivity)\n",
    "\tspecificity_list.append(specificity)\n",
    "\tprecision_list.append(precision)\n",
    "\tf1_score_list.append(f1_score)\n",
    "\tauroc_list.append(auroc)\n",
    "\taupr_list.append(aupr)\n",
    "\t\n",
    "\n",
    "\n",
    "for i in range(len(final_predictions)):\n",
    "\tif final_predictions[i] >=5:\n",
    "\t\tfinal_predictions[i]=1\n",
    "\telse:\n",
    "\t\tfinal_predictions[i]=0\n",
    "\n",
    "\n",
    "correct_predictions = np.sum(final_predictions == Testset_Y.to_numpy())\n",
    "accuracy = correct_predictions / Testset_Y.shape[0]\n",
    "sensitivity = recall_score(Testset_Y, final_predictions)\n",
    "print(\"Final Accuracy:\", accuracy)\n",
    "print(\"final Sensitivity: \", sensitivity)\n",
    "print(sensitivity_list)\n",
    "\n",
    "stacked_predictions_1 = np.hstack(Prediction_list)\n",
    "mean_predictions = np.mean(stacked_predictions_1, axis=1)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(Testset_Y, final_predictions, mean_predictions)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accuracy, sensitivity, specificity, precision, f1_score, auroc, aupr])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accu_mean,sens_mean, spec_mean, pre_mean, f1_mean, auroc_mean, aupr_mean = get_mean_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list)\n",
    "accu_std,sens_std, spec_std, pre_std, f1_std, auroc_std, aupr_std = get_stdev_values(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list)\n",
    "print(\"Dataset-3 Mean Scores for Bagging:\")\n",
    "print(\"Accuracy: \", accu_mean, \"Sensitivity: \", sens_mean, \"Specificity: \", spec_mean, \"Precision: \", pre_mean, \"F1_score: \", f1_score, \"AUROC: \", auroc_mean, \"AUPR: \", aupr_mean)\n",
    "print(\"Dataset-3 Standard Deviations of Metrics for Bagging:\")\n",
    "print(\"Accuracy: \", accu_std, \"Sensitivity: \", sens_std, \"Specificity: \", spec_std, \"Precision: \", pre_std, \"F1_score: \", f1_std, \"AUROC: \", auroc_std, \"AUPR: \", aupr_std)\n",
    "main_list.append([accu_mean,sens_mean, spec_mean, pre_mean, f1_mean, auroc_mean, aupr_mean])\n",
    "main_list.append([accu_std,sens_std, spec_std, pre_std, f1_std, auroc_std, aupr_std])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violins(accuracy_list, sensitivity_list, specificity_list, precision_list, f1_score_list, auroc_list, aupr_list,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing the validation data for input in stacking\n",
    "df_X = pd.DataFrame(np.vstack(validation_X_list))\n",
    "df_Y = pd.DataFrame(np.vstack(validation_Y_list),columns=[\"Y\"])\n",
    "merged_dataframe = pd.concat([df_X, df_Y], axis=1)\n",
    "merged_dataframe = merged_dataframe.drop_duplicates()\n",
    "final_X = merged_dataframe.drop(columns=[\"Y\"]).to_numpy() \n",
    "final_Y = merged_dataframe[\"Y\"].to_numpy().reshape(-1,1)\n",
    "print(\"Final X shape:\", final_X.shape)\n",
    "print(\"Final Y shape:\", final_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_stacking_X = pd.DataFrame(final_X)\n",
    "\n",
    "for i in range(0,9):\n",
    "\taccuracy, predictions,_ = get_accuracy_results(final_X, final_Y, W_list[i])\n",
    "\tdf_temp = pd.DataFrame(predictions)\n",
    "\tdataset_stacking_X = pd.concat([dataset_stacking_X, df_temp], axis=1)\n",
    "\n",
    "dataset_stacking_Y = pd.DataFrame(final_Y)\n",
    "print(dataset_stacking_X.shape)\n",
    "print(dataset_stacking_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_train_X_stacking_2, init_train_Y_stacking_2, test_X_stacking_2, test_Y_stacking_2  = dataset_splitting(dataset_stacking_X, dataset_stacking_Y, 0.2, 90)\n",
    "train_X_stacking_2, train_Y_stacking_2, validation_X_stacking_2, validation_Y_stacking_2 = dataset_splitting(pd.DataFrame(init_train_X_stacking_2), pd.DataFrame(init_train_Y_stacking_2), 0.2, 39)\n",
    "W_stacking_2, alpha_stacking_2, itr_stacking_2 = cross_validation(train_X_stacking_2, train_Y_stacking_2, validation_X_stacking_2, validation_Y_stacking_2, 48)\n",
    "accu_stacking_3, binary_predictions_stacking_2, predictions_stacking_2 = get_accuracy_results(test_X_stacking_2, test_Y_stacking_2, W_stacking_2)\n",
    "sensitivity, specificity, precision, f1_score, auroc, aupr = getParameters(test_Y_stacking_2, binary_predictions_stacking_2, predictions_stacking_2)\n",
    "print(\"accuracy: \", accu_stacking_3, \"optimal_alpha: \", alpha_stacking_2, \"optimal_iteration_num: \", itr_stacking_2)\n",
    "print(sensitivity, specificity, precision, f1_score, auroc, aupr)\n",
    "main_list.append([accu_stacking_3, sensitivity, specificity, precision, f1_score, auroc, aupr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(main_list, columns=['Accuracy', 'Sensitivity', 'Specificity', 'Precision', 'F1 Score', 'AUROC', 'AUPR'])\n",
    "df.to_csv('metrics_summary.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
